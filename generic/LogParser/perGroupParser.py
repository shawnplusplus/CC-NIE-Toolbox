#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""@package perGroupParser
Copyright (C) 2015 University of Virginia. All rights reserved.

file      perGroupParser.py
author    Shawn Chen <sc7cq@virginia.edu>
version   1.0
date      June 27, 2015

LICENSE

This program is free software; you can redistribute it and/or modify it
under the terms of the GNU General Public License as published by the Free
Software Foundation; either version 2 of the License, or（at your option）
any later version.

This program is distributed in the hope that it will be useful, but WITHOUT
ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for
more details at http://www.gnu.org/copyleft/gpl.html

brief     parses log files generated by VCMTPv3 receivers on a specified
          aggregate size basis.
usage     python perGroupParser.py <metadata> <logfile> <csvfile-to-write>
"""


from __future__ import division
import csv
import re
import sys


def parseSizeTime(line):
    """Parses the product size and receiving time in a line.

    Parses the product size and receiving time consumed for the product in
    the given line of log file.

    Args:
        line: A line of the raw log file.

    Returns:
        (-1, -1, -1): If no valid size or time is found.
        (prodindex, prodsize, rxtime): A tuple of product index, product size
                                       and receiving time.
    """
    success_match = re.search(r'.*\[SUCCESS\].*#(\d+)', line)
    if success_match:
        sizematch = re.search(r'.*size = (\d+) bytes', line)
        timematch = re.search(r'.*time = (\d+\.\d+) seconds', line)
    if success_match and sizematch and timematch:
        prodindex = int(success_match.group(1))
        size      = int(sizematch.group(1))
        rxtime    = float(timematch.group(1))
        return (prodindex, size, rxtime)
    else:
        return (-1, -1, -1)


def parseFailure(line):
    """Parses the log to find unsuccessfully received products.

    Parses the failure message in log to find unsuccessfully received products,
    which are caused by retransmission timeouts. Since we are not able to tell
    whether a product is failed or simply out-of-sequence, we cannot detect
    failure by looking at product indices. The only reliable way is the failure
    message in log file.

    Args:
        line: A line of the raw log file.

    Returns:
        -1: If no failure message is found.
        prodindex: Index of the product which is indicated lost.
    """
    failure_match = re.search(r'.*\[FAILURE\].*#(\d+)', line)
    if failure_match:
        return int(failure_match.group(1))
    else:
        return -1


def parseMcastData(line):
    """Parses the log to find multicast data blocks.

    Parses the log to find multicast data blocks.

    Args:
        line: A line of the raw log file.

    Returns:
        -1: If no multicast block is found.
        prodindex: Index of the product which has multicast blocks.
    """
    mcast_match = re.search(r'.*\[MCAST DATA\].*#(\d+)', line)
    if mcast_match:
        return int(mcast_match.group(1))
    else:
        return -1


def parseRetxData(line):
    """Parses the log to find retransmitted data blocks.

    Parses the log to find retransmitted data blocks.

    Args:
        line: A line of the raw log file.

    Returns:
        -1: If no retransmitted block is found.
        prodindex: Index of the product which has retransmitted blocks.
    """
    retx_match = re.search(r'.*\[RETX DATA\].*#(\d+)', line)
    if retx_match:
        return int(retx_match.group(1))
    else:
        return -1


def aggregate(filename, aggregate_size):
    """Does aggregating on the given input csv.

    Does size aggregating over a csv file that contains sizes of products in
    the first column and returns the aggregate results.

    Args:
        filename: Filename of the csv file.
        aggregate_size: The size of each aggregate group.

    Returns:
        (groups, sizes): A list of aggregated groups.
    """
    groups   = []
    group    = []
    sizes    = []
    sum_size = 0
    with open(filename, 'rb') as csvfile:
        csvreader = csv.reader(csvfile, delimiter=',')
        for i, row in enumerate(csvreader):
            group.append(i)
            sum_size += int(row[0])
            if sum_size >= aggregate_size:
                groups.append(group)
                sizes.append(sum_size)
                group    = []
                sum_size = 0
        if group and sum_size:
            groups.append(group)
            sizes.append(sum_size)
    csvfile.close()
    return (groups, sizes)


def extractLog(filename):
    """Classifies the log file and extracts the lossless, complete and failed
    products.

    Classifies and extracts the lossless, complete and failed products
    separately and saves in different lists.

    Args:
        filename: Filename of the log file.

    Returns:
        (lossless, complete_set, complete_dict, failed, mcast, retx):
        extracted groups.
    """
    # lossless is the group endured no loss.
    lossless = set()
    # complete is the group completely received products with or without loss.
    complete_set  = set()
    complete_dict = {}
    # failed is the group of not completely received products.
    failed   = set()
    # mcast is a dict containing the mcast blocks of each product.
    mcast    = {}
    # retx is a dict containing the retx blocks of each product.
    retx     = {}
    with open(filename, 'r') as logfile:
        for i, line in enumerate(logfile):
            (prodid_success, size, rxtime) = parseSizeTime(line)
            prodid_failure = parseFailure(line)
            prodid_mcast = parseMcastData(line)
            prodid_retx = parseRetxData(line)
            if prodid_success >= 0:
                complete_set |= {prodid_success}
                if not complete_dict.has_key(prodid_success):
                    complete_dict[prodid_success] = (size, rxtime)
            if prodid_failure >= 0:
                failed |= {prodid_failure}
            if prodid_mcast >= 0:
                if mcast.has_key(prodid_mcast):
                    mcast[prodid_mcast] += 1
                else:
                    mcast[prodid_mcast] = 1
            if prodid_retx >= 0:
                if retx.has_key(prodid_retx):
                    retx[prodid_retx] += 1
                else:
                    retx[prodid_retx] = 1
    logfile.close()
    retx_set = set(retx.keys())
    lossless = complete_set - retx_set
    return (lossless, complete_set, complete_dict, failed, mcast, retx)


def calcThroughput(tx_group, lossless, complete_set, complete_dict):
    """Calculates throughput for an aggregate.

    Calculates throughput for a lossless group which fits into the aggregate
    size and also cumulates the group size.

    Args:
        tx_group: Aggregate group.
        lossless: Group of lossless products.
        complete_set: Set of complete products.
        complete_dict: Dict of complete products.

    Returns:
        (thru_lossless, thru_complete, complete_size): calculated throughputs.
    """
    lossless_size = 0
    lossless_time = 0
    complete_size = 0
    complete_time = 0
    thru_lossless = 0
    thru_complete = 0
    for i in tx_group & complete_set:
        complete_size += complete_dict[i][0]
        complete_time += complete_dict[i][1]
        if i in tx_group & lossless:
            lossless_size += complete_dict[i][0]
            lossless_time += complete_dict[i][1]
    if lossless_time:
        thru_lossless = float(lossless_size / lossless_time) * 8
    else:
        thru_lossless = -1
    if complete_time:
        thru_complete = float(complete_size / complete_time) * 8
    else:
        thru_complete = -1
    return (thru_lossless, thru_complete, complete_size)


def calcRatio(tx_group, lossless, complete_set, failed):
    """Calculates ratios for an aggregate.

    Calculates ratios including lossless ratio, complete ratio and block retx
    ratio in an aggregate.

    Args:
        tx_group: Aggregate group.
        lossless: Group of lossless products.
        complete_set: Set of complete products.
        failed: Group of failed products.

    Returns:
        (lossless_ratio, complete_ratio, failed_ratio): calculated ratios.
    """
    aggregate_num = len(tx_group)
    lossless_num  = len(tx_group & lossless)
    complete_num  = len(tx_group & complete_set)
    failed_num    = len(tx_group & failed)
    if aggregate_num:
        lossless_ratio = float(lossless_num / aggregate_num) * 100
        complete_ratio = float(complete_num / aggregate_num) * 100
        failed_ratio   = float(failed_num / aggregate_num) * 100
    else:
        lossless_ratio = -1
        complete_ratio = -1
        failed_ratio   = -1
    return (lossless_ratio, complete_ratio, failed_ratio)


def calcBlockRetxRate(tx_group, mcast, retx):
    """Calculates block retx ratio in an aggregate.

    Args:
        tx_group: Aggregate group.
        mcast: Dict of mcast products.
        retx: Dict of retx products.

    Returns:
        (lossless_ratio, complete_ratio, failed_ratio): calculated ratios.
    """
    mcast_num = 0
    retx_num  = 0
    for i in tx_group:
        if mcast.has_key(i):
            mcast_num += mcast[i]
        if retx.has_key(i):
            retx_num += retx[i]
    if mcast_num + retx_num:
        retx_block_rate = float(retx_num / (mcast_num + retx_num)) * 100
    else:
        retx_block_rate = -1
    return retx_block_rate


def main(metadata, logfile, csvfile):
    """Reads the raw log file and parses it.

    Reads the raw VCMTPv3 log file, parses each line and computes throughput,
    block-based retransmission rate and FDSR over an aggregate size.

    Args:
        metadata: Filename of the metadata.
        logfile: Filename of the log file.
        csvfile : Filename of the new file to contain output results.
    """
    w = open(csvfile, 'w+')
    aggregate_size = 10 * 1024 * 1024
    (tx_groups, tx_sizes) = aggregate(metadata, aggregate_size)
    (rx_noloss, rx_success_set, rx_success_dict, rx_failed, rx_mcast,
     rx_retx) = extractLog(logfile)
    tmp_str = 'Sent first prodindex, Sent last prodindex, Sender aggregate ' \
              'size (B), Successfully received aggregate size (B), Lossless ' \
              'throughput (bps), Successful throughput (bps), Ratio of ' \
              'lossless products (%), Number of lossless products, Ratio of ' \
              'successful products (%), Number of successful products, Ratio ' \
              'of failed products (%), Number of failed products, Block ' \
              'retransmission rate (%)' + '\n'
    w.write(tmp_str)
    for group, size in zip(tx_groups, tx_sizes):
        (thru_lossless, thru_complete, rx_group_size) = calcThroughput(
            set(group), rx_noloss, rx_success_set, rx_success_dict)
        (lossless_ratio, complete_ratio, failed_ratio) = calcRatio(set(group),
            rx_noloss, rx_success_set, rx_failed)
        lossless_num = len(set(group) & rx_noloss)
        complete_num = len(set(group) & rx_success_set)
        failed_num   = len(set(group) & rx_failed)
        retx_block_rate = calcBlockRetxRate(set(group), rx_mcast, rx_retx)
        tmp_str = str(min(group)) + ',' + str(max(group)) + ',' \
                + str(size) + ',' + str(rx_group_size) + ',' \
                + str(thru_lossless) + ',' + str(thru_complete) + ',' \
                + str(lossless_ratio) + ',' + str(lossless_num) + ',' \
                + str(complete_ratio) + ',' + str(complete_num) + ',' \
                + str(failed_ratio) + ',' + str(failed_num) + ',' \
                + str(retx_block_rate) + '\n'
        w.write(tmp_str)
    w.close()


if __name__ == "__main__":
    main(sys.argv[1], sys.argv[2], sys.argv[3])
